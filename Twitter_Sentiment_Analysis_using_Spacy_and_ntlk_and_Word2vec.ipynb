{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPIihgXlK8mWLawEpRoEiOo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhosrojerdiA/NLP/blob/main/Twitter_Sentiment_Analysis_using_Spacy_and_ntlk_and_Word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qZ2xXHgu3rx"
      },
      "outputs": [],
      "source": [
        "# DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "# Matplot\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "from keras import utils\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "import spacy\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from  nltk.stem import SnowballStemmer\n",
        "\n",
        "# Word2vec\n",
        "import gensim\n",
        "\n",
        "# Utility\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Hi! My name is Spencer and I love doing videos related to tech implementation/strategy. If you like finance, tech, or real world\n",
        "Q&A videos about Data Science, Data Engineering, and Data Analyst related topics, let me know in the comments down below! *psst*\n",
        "If you made it this far in the video, why don't you subscribe? I upload videos every week on a Sunday ~7:00 PM EST. If you like\n",
        "this type of content make sure to hit that like button! That really helps out with the growth of this channel. :)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "A2TfOtGfu8rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Process Text"
      ],
      "metadata": {
        "id": "cRZgHYL_vBDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r'[^A-Za-z ]'\n",
        "regex = re.compile(pattern)\n",
        "result = regex.sub('', text)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "9EZf5VRxvDkm",
        "outputId": "dae1bcfc-1f64-4ddc-874d-45c18d07cba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi My name is Spencer and I love doing videos related to tech implementationstrategy If you like finance tech or real worldQA videos about Data Science Data Engineering and Data Analyst related topics let me know in the comments down below psstIf you made it this far in the video why dont you subscribe I upload videos every week on a Sunday  PM EST If you likethis type of content make sure to hit that like button That really helps out with the growth of this channel '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the NLP model that you have chosen to downloaded; I have the large model.\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(result)\n",
        "\n",
        "# Let's get each individual word as an element.\n",
        "tokens = [token for token in doc]\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04owfMTbvKDz",
        "outputId": "4794ae6f-8446-4cab-efca-9aedc00990f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Hi,\n",
              " My,\n",
              " name,\n",
              " is,\n",
              " Spencer,\n",
              " and,\n",
              " I,\n",
              " love,\n",
              " doing,\n",
              " videos,\n",
              " related,\n",
              " to,\n",
              " tech,\n",
              " implementationstrategy,\n",
              " If,\n",
              " you,\n",
              " like,\n",
              " finance,\n",
              " tech,\n",
              " or,\n",
              " real,\n",
              " worldQA,\n",
              " videos,\n",
              " about,\n",
              " Data,\n",
              " Science,\n",
              " Data,\n",
              " Engineering,\n",
              " and,\n",
              " Data,\n",
              " Analyst,\n",
              " related,\n",
              " topics,\n",
              " let,\n",
              " me,\n",
              " know,\n",
              " in,\n",
              " the,\n",
              " comments,\n",
              " down,\n",
              " below,\n",
              " psstIf,\n",
              " you,\n",
              " made,\n",
              " it,\n",
              " this,\n",
              " far,\n",
              " in,\n",
              " the,\n",
              " video,\n",
              " why,\n",
              " do,\n",
              " nt,\n",
              " you,\n",
              " subscribe,\n",
              " I,\n",
              " upload,\n",
              " videos,\n",
              " every,\n",
              " week,\n",
              " on,\n",
              " a,\n",
              " Sunday,\n",
              "  ,\n",
              " PM,\n",
              " EST,\n",
              " If,\n",
              " you,\n",
              " likethis,\n",
              " type,\n",
              " of,\n",
              " content,\n",
              " make,\n",
              " sure,\n",
              " to,\n",
              " hit,\n",
              " that,\n",
              " like,\n",
              " button,\n",
              " That,\n",
              " really,\n",
              " helps,\n",
              " out,\n",
              " with,\n",
              " the,\n",
              " growth,\n",
              " of,\n",
              " this,\n",
              " channel]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t in tokens:\n",
        "    print('Token is : ', t,'--- Is this a stop word? ', t.is_stop, '--- Lemmatized token is: ', t.lemma_)\n",
        "\n",
        "# Store the lemmas without the words.\n",
        "lemmas = [t.lemma_ for t in tokens if not t.is_stop]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0sFxdTAviJp",
        "outputId": "9fc6163a-6905-480f-bfaa-9de26069b836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is :  Hi --- Is this a stop word?  False --- Lemmatized token is:  hi\n",
            "Token is :  My --- Is this a stop word?  True --- Lemmatized token is:  my\n",
            "Token is :  name --- Is this a stop word?  True --- Lemmatized token is:  name\n",
            "Token is :  is --- Is this a stop word?  True --- Lemmatized token is:  be\n",
            "Token is :  Spencer --- Is this a stop word?  False --- Lemmatized token is:  Spencer\n",
            "Token is :  and --- Is this a stop word?  True --- Lemmatized token is:  and\n",
            "Token is :  I --- Is this a stop word?  True --- Lemmatized token is:  I\n",
            "Token is :  love --- Is this a stop word?  False --- Lemmatized token is:  love\n",
            "Token is :  doing --- Is this a stop word?  True --- Lemmatized token is:  do\n",
            "Token is :  videos --- Is this a stop word?  False --- Lemmatized token is:  video\n",
            "Token is :  related --- Is this a stop word?  False --- Lemmatized token is:  relate\n",
            "Token is :  to --- Is this a stop word?  True --- Lemmatized token is:  to\n",
            "Token is :  tech --- Is this a stop word?  False --- Lemmatized token is:  tech\n",
            "Token is :  implementationstrategy --- Is this a stop word?  False --- Lemmatized token is:  implementationstrategy\n",
            "Token is :  If --- Is this a stop word?  True --- Lemmatized token is:  if\n",
            "Token is :  you --- Is this a stop word?  True --- Lemmatized token is:  you\n",
            "Token is :  like --- Is this a stop word?  False --- Lemmatized token is:  like\n",
            "Token is :  finance --- Is this a stop word?  False --- Lemmatized token is:  finance\n",
            "Token is :  tech --- Is this a stop word?  False --- Lemmatized token is:  tech\n",
            "Token is :  or --- Is this a stop word?  True --- Lemmatized token is:  or\n",
            "Token is :  real --- Is this a stop word?  False --- Lemmatized token is:  real\n",
            "Token is :  worldQA --- Is this a stop word?  False --- Lemmatized token is:  worldQA\n",
            "Token is :  videos --- Is this a stop word?  False --- Lemmatized token is:  video\n",
            "Token is :  about --- Is this a stop word?  True --- Lemmatized token is:  about\n",
            "Token is :  Data --- Is this a stop word?  False --- Lemmatized token is:  Data\n",
            "Token is :  Science --- Is this a stop word?  False --- Lemmatized token is:  Science\n",
            "Token is :  Data --- Is this a stop word?  False --- Lemmatized token is:  Data\n",
            "Token is :  Engineering --- Is this a stop word?  False --- Lemmatized token is:  Engineering\n",
            "Token is :  and --- Is this a stop word?  True --- Lemmatized token is:  and\n",
            "Token is :  Data --- Is this a stop word?  False --- Lemmatized token is:  Data\n",
            "Token is :  Analyst --- Is this a stop word?  False --- Lemmatized token is:  Analyst\n",
            "Token is :  related --- Is this a stop word?  False --- Lemmatized token is:  relate\n",
            "Token is :  topics --- Is this a stop word?  False --- Lemmatized token is:  topic\n",
            "Token is :  let --- Is this a stop word?  False --- Lemmatized token is:  let\n",
            "Token is :  me --- Is this a stop word?  True --- Lemmatized token is:  I\n",
            "Token is :  know --- Is this a stop word?  False --- Lemmatized token is:  know\n",
            "Token is :  in --- Is this a stop word?  True --- Lemmatized token is:  in\n",
            "Token is :  the --- Is this a stop word?  True --- Lemmatized token is:  the\n",
            "Token is :  comments --- Is this a stop word?  False --- Lemmatized token is:  comment\n",
            "Token is :  down --- Is this a stop word?  True --- Lemmatized token is:  down\n",
            "Token is :  below --- Is this a stop word?  True --- Lemmatized token is:  below\n",
            "Token is :  psstIf --- Is this a stop word?  False --- Lemmatized token is:  psstIf\n",
            "Token is :  you --- Is this a stop word?  True --- Lemmatized token is:  you\n",
            "Token is :  made --- Is this a stop word?  True --- Lemmatized token is:  make\n",
            "Token is :  it --- Is this a stop word?  True --- Lemmatized token is:  it\n",
            "Token is :  this --- Is this a stop word?  True --- Lemmatized token is:  this\n",
            "Token is :  far --- Is this a stop word?  False --- Lemmatized token is:  far\n",
            "Token is :  in --- Is this a stop word?  True --- Lemmatized token is:  in\n",
            "Token is :  the --- Is this a stop word?  True --- Lemmatized token is:  the\n",
            "Token is :  video --- Is this a stop word?  False --- Lemmatized token is:  video\n",
            "Token is :  why --- Is this a stop word?  True --- Lemmatized token is:  why\n",
            "Token is :  do --- Is this a stop word?  True --- Lemmatized token is:  do\n",
            "Token is :  nt --- Is this a stop word?  False --- Lemmatized token is:  not\n",
            "Token is :  you --- Is this a stop word?  True --- Lemmatized token is:  you\n",
            "Token is :  subscribe --- Is this a stop word?  False --- Lemmatized token is:  subscribe\n",
            "Token is :  I --- Is this a stop word?  True --- Lemmatized token is:  I\n",
            "Token is :  upload --- Is this a stop word?  False --- Lemmatized token is:  upload\n",
            "Token is :  videos --- Is this a stop word?  False --- Lemmatized token is:  video\n",
            "Token is :  every --- Is this a stop word?  True --- Lemmatized token is:  every\n",
            "Token is :  week --- Is this a stop word?  False --- Lemmatized token is:  week\n",
            "Token is :  on --- Is this a stop word?  True --- Lemmatized token is:  on\n",
            "Token is :  a --- Is this a stop word?  True --- Lemmatized token is:  a\n",
            "Token is :  Sunday --- Is this a stop word?  False --- Lemmatized token is:  Sunday\n",
            "Token is :    --- Is this a stop word?  False --- Lemmatized token is:   \n",
            "Token is :  PM --- Is this a stop word?  False --- Lemmatized token is:  pm\n",
            "Token is :  EST --- Is this a stop word?  False --- Lemmatized token is:  EST\n",
            "Token is :  If --- Is this a stop word?  True --- Lemmatized token is:  if\n",
            "Token is :  you --- Is this a stop word?  True --- Lemmatized token is:  you\n",
            "Token is :  likethis --- Is this a stop word?  False --- Lemmatized token is:  likethis\n",
            "Token is :  type --- Is this a stop word?  False --- Lemmatized token is:  type\n",
            "Token is :  of --- Is this a stop word?  True --- Lemmatized token is:  of\n",
            "Token is :  content --- Is this a stop word?  False --- Lemmatized token is:  content\n",
            "Token is :  make --- Is this a stop word?  True --- Lemmatized token is:  make\n",
            "Token is :  sure --- Is this a stop word?  False --- Lemmatized token is:  sure\n",
            "Token is :  to --- Is this a stop word?  True --- Lemmatized token is:  to\n",
            "Token is :  hit --- Is this a stop word?  False --- Lemmatized token is:  hit\n",
            "Token is :  that --- Is this a stop word?  True --- Lemmatized token is:  that\n",
            "Token is :  like --- Is this a stop word?  False --- Lemmatized token is:  like\n",
            "Token is :  button --- Is this a stop word?  False --- Lemmatized token is:  button\n",
            "Token is :  That --- Is this a stop word?  True --- Lemmatized token is:  that\n",
            "Token is :  really --- Is this a stop word?  True --- Lemmatized token is:  really\n",
            "Token is :  helps --- Is this a stop word?  False --- Lemmatized token is:  help\n",
            "Token is :  out --- Is this a stop word?  True --- Lemmatized token is:  out\n",
            "Token is :  with --- Is this a stop word?  True --- Lemmatized token is:  with\n",
            "Token is :  the --- Is this a stop word?  True --- Lemmatized token is:  the\n",
            "Token is :  growth --- Is this a stop word?  False --- Lemmatized token is:  growth\n",
            "Token is :  of --- Is this a stop word?  True --- Lemmatized token is:  of\n",
            "Token is :  this --- Is this a stop word?  True --- Lemmatized token is:  this\n",
            "Token is :  channel --- Is this a stop word?  False --- Lemmatized token is:  channel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Twitter Data"
      ],
      "metadata": {
        "id": "EGIOHN-KvuDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading in twitter data on sentiment. (NEGATIVE, POSITIVE for target)\n",
        "# Already cleaned and preprocessed...\n",
        "df = pd.read_csv('twitter_data.csv')\n",
        "df = df.sample(frac=1).reset_index()\n",
        "df = df.drop(['index'], axis = 1)\n",
        "df"
      ],
      "metadata": {
        "id": "m6Y9vBPQvvan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Process Data"
      ],
      "metadata": {
        "id": "ncpXyFdnX1cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def preprocess(text, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\", ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "z4QHdtCiv00T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Apply"
      ],
      "metadata": {
        "id": "ibrpgbODX34t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.text = df.text.apply(lambda x: preprocess(x)) # preprocessing the text data."
      ],
      "metadata": {
        "id": "X-GzaG95v3h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Check"
      ],
      "metadata": {
        "id": "DP_sPUyoX7IO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.text[0]"
      ],
      "metadata": {
        "id": "ni3Jbuyev4z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split"
      ],
      "metadata": {
        "id": "7Q34EyCmX8pQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test dataset.\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "print(\"TRAIN size:\", len(df_train))\n",
        "print(\"TEST size:\", len(df_test))"
      ],
      "metadata": {
        "id": "raOJc8Lxv_LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2vec"
      ],
      "metadata": {
        "id": "DRLrhJuawFCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [_text.split() for _text in df_train.text]\n",
        "\n",
        "w2v_model = gensim.models.word2vec.Word2Vec(vector_size =300, # vector size\n",
        "                                            window=7, # distance between current and predicted word within a sentence\n",
        "                                            min_count=10, # ignores words with total frequency less than the parameter\n",
        "                                            workers=8) # threads\n",
        "w2v_model.build_vocab(documents)\n",
        "words = w2v_model.wv.index_to_key\n",
        "vocab_size = len(words)\n",
        "print(\"Vocab size\", vocab_size)"
      ],
      "metadata": {
        "id": "p_nS-jDWwGuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.train(documents, total_examples=len(documents), epochs=32)"
      ],
      "metadata": {
        "id": "OQ1cXOTbwPS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(\"like\")\n",
        "\n",
        "\n",
        "#[('horrible', 0.3705178201198578),\n",
        "# ('creepy', 0.3669455945491791),\n",
        "# ('disgusting', 0.3664887547492981),\n",
        "# ('understand', 0.3367389440536499),\n",
        "# ('awful', 0.33474934101104736),\n",
        "# ('guilty', 0.33130747079849243),\n",
        "# ('weird', 0.3161929249763489),\n",
        "# ('tells', 0.30077123641967773),\n",
        "# ('worse', 0.2977883815765381),\n",
        "# ('interesting', 0.2937477231025696)]"
      ],
      "metadata": {
        "id": "vXj6SxRywUt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(\"comment\")\n",
        "#('link', 0.4997900128364563),\n",
        "# ('commented', 0.48193472623825073),\n",
        "# ('blogtv', 0.45630455017089844),\n",
        "# ('comments', 0.4551774561405182),\n",
        "# ('flickr', 0.4323364198207855),\n",
        "# ('suscribe', 0.43144938349723816"
      ],
      "metadata": {
        "id": "Jdjw6Z1UwYlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenize Text & Create Embedding Layer.\n",
        "using word2vec"
      ],
      "metadata": {
        "id": "yNDk1-S7wlBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv('twitter_data.csv')\n",
        "#df = df.sample(frac=1).reset_index()\n",
        "#df = df.drop(['index'], axis = 1)\n",
        "#df\n",
        "\n",
        "# Split into train and test dataset.\n",
        "#df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "bw2iFWrqYnyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tokenizer = Tokenizer() #from Keras\n",
        "tokenizer.fit_on_texts(df_train.text)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Total words\", vocab_size)"
      ],
      "metadata": {
        "id": "_cHFkMjcwmj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text to Sequence\n",
        "New Train and Test"
      ],
      "metadata": {
        "id": "Cu6GJJH1xw5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=300)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=300)"
      ],
      "metadata": {
        "id": "kIgL0GabwuBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train), len(x_test))"
      ],
      "metadata": {
        "id": "B78FxCzcwvIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating an embedding layer\n",
        "that will act as an input layer for the neural network."
      ],
      "metadata": {
        "id": "HEoD4s3OYRXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "print(embedding_matrix.shape)\n",
        "# used in the future.\n",
        "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)"
      ],
      "metadata": {
        "id": "AsKVvYMjwwA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df_train.target.unique().tolist()\n",
        "labels\n",
        "\n",
        "#['NEGATIVE', 'POSITIVE']"
      ],
      "metadata": {
        "id": "8ozTUR2UwzHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Label encoding of y_train and y_test"
      ],
      "metadata": {
        "id": "jYxehEbLY-DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(df_train.target.tolist())\n",
        "\n",
        "y_train = encoder.transform(df_train.target.tolist())\n",
        "y_test = encoder.transform(df_test.target.tolist())\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "print(\"y_train\",y_train.shape)\n",
        "print(\"y_test\",y_test.shape)\n",
        "y_train (80000, 1)\n",
        "y_test (20000, 1)"
      ],
      "metadata": {
        "id": "OFqYAUthw137"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "oglTNGgMxA5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) # This can be BERT which may have better performance.\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "b1MKwtiwxCEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=\"adam\",\n",
        "              metrics=['accuracy'])\n",
        "callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
        "              EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]"
      ],
      "metadata": {
        "id": "q_tW6oiixDPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    batch_size=1024,\n",
        "                    epochs=8,\n",
        "                    validation_split=0.1,\n",
        "                    verbose=1,\n",
        "                    callbacks = callbacks)"
      ],
      "metadata": {
        "id": "qaYFY9ycxFYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate"
      ],
      "metadata": {
        "id": "YOog0qQpxK2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print()\n",
        "print(\"ACCURACY:\",score[1])\n",
        "print(\"LOSS:\",score[0])"
      ],
      "metadata": {
        "id": "RQsOC6ypxML0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show() # Seems like the model may be overfitting. training loss << validation loss"
      ],
      "metadata": {
        "id": "2wOUurgKxONa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predict on a sentence"
      ],
      "metadata": {
        "id": "lNPa5MG8xUd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sentiment(score):\n",
        "    return 'NEGATIVE' if score < 0.5 else 'POSITIVE'\n",
        "\n",
        "def predict(text, include_neutral=True):\n",
        "    start_at = time.time()\n",
        "    # Tokenize text\n",
        "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=300)\n",
        "    # Predict\n",
        "    score = model.predict([x_test])[0]\n",
        "#     print(score)\n",
        "    # Decode sentiment\n",
        "    label = decode_sentiment(score[0])\n",
        "\n",
        "    return {\"label\": label, \"score\": float(score),\n",
        "       \"elapsed_time\": time.time()-start_at}"
      ],
      "metadata": {
        "id": "GdRw8Le0xWoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(\"Leave a like on this video, comment, and subscribe for more!\")\n",
        "\n",
        "#{'label': 'POSITIVE',\n",
        "# 'score': 0.6529418230056763,\n",
        "# 'elapsed_time': 0.08769869804382324}"
      ],
      "metadata": {
        "id": "8a5FwWiMxel0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}